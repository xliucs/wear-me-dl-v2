#!/usr/bin/env python3
"""
V12: Fully exploit sample weighting discovery.

V11 breakthrough: sqrt(y) weighting gave +0.008 R² on single model.
Now: apply optimal weighting to ALL model types + fine-tune weight function.

Strategy:
1. Fine-tune weight exponent: y^0.2, y^0.3, y^0.4, y^0.5, y^0.6, y^0.7
2. Apply best weighting to ALL model families (XGB, LGB, HGBR, ElasticNet, Ridge)
3. Apply to ALL feature sets (v7, eng)
4. Apply to ALL target transforms (log, sqrt, raw)
5. Mega-blend with greedy forward + Dirichlet
"""
import numpy as np, pandas as pd, time, warnings, sys, json
warnings.filterwarnings('ignore')
from eval_framework import (load_data, get_feature_sets, get_cv_splits,
                             engineer_all_features)
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge
from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor
from sklearn.metrics import r2_score
import xgboost as xgb
import lightgbm as lgb

t_start = time.time()
print("="*60)
print("  V12: WEIGHT EXPLOITATION")
print("="*60)
sys.stdout.flush()

X_df, y, fn = load_data()
X_all_raw, _, all_cols, _ = get_feature_sets(X_df)
splits = get_cv_splits(y)
n = len(y)

def eng_v7(X_df, cols):
    X = X_df[cols].copy()
    g=X['glucose'].clip(lower=1); t=X['triglycerides'].clip(lower=1)
    h=X['hdl'].clip(lower=1); b=X['bmi']; l=X['ldl']
    tc=X['total cholesterol']; nh=X['non hdl']; ch=X['chol/hdl']; a=X['age']
    for nm,v in [('tyg',np.log(t*g/2)),('tyg_bmi',np.log(t*g/2)*b),
        ('mets_ir',np.log(2*g+t)*b/np.log(h)),('trig_hdl',t/h),('trig_hdl_log',np.log1p(t/h)),
        ('vat_proxy',b*t/h),('ir_proxy',g*b*t/(h*100)),('glucose_bmi',g*b),
        ('glucose_sq',g**2),('glucose_log',np.log(g)),('glucose_hdl',g/h),
        ('glucose_trig',g*t/1000),('glucose_non_hdl',g*nh/100),('glucose_chol_hdl',g*ch),
        ('bmi_sq',b**2),('bmi_log',np.log(b.clip(lower=1))),('bmi_trig',b*t/100),
        ('bmi_hdl_inv',b/h),('bmi_age',b*a),('ldl_hdl',l/h),('non_hdl_ratio',nh/h),
        ('tc_hdl_bmi',tc/h*b),('trig_tc',t/tc.clip(lower=1))]:
        X[nm] = v
    X['tyg_sq']=X['tyg']**2; X['mets_ir_sq']=X['mets_ir']**2
    X['trig_hdl_sq']=X['trig_hdl']**2; X['vat_sq']=X['vat_proxy']**2
    X['ir_proxy_sq']=X['ir_proxy']**2; X['ir_proxy_log']=np.log1p(X['ir_proxy'])
    rhr='Resting Heart Rate (mean)'; hrv='HRV (mean)'; stp='STEPS (mean)'
    if rhr in X.columns:
        for nm,v in [('bmi_rhr',b*X[rhr]),('glucose_rhr',g*X[rhr]),
            ('trig_hdl_rhr',X['trig_hdl']*X[rhr]),('ir_proxy_rhr',X['ir_proxy']*X[rhr]/100),
            ('tyg_rhr',X['tyg']*X[rhr]),('mets_rhr',X['mets_ir']*X[rhr]),
            ('bmi_hrv_inv',b/X[hrv].clip(lower=1)),
            ('cardio_fitness',X[hrv]*X[stp]/X[rhr].clip(lower=1)),
            ('met_load',b*X[rhr]/X[stp].clip(lower=1)*1000)]:
            X[nm] = v
        for pfx,m,s in [('rhr',rhr,'Resting Heart Rate (std)'),('hrv',hrv,'HRV (std)'),
                         ('stp',stp,'STEPS (std)'),('slp','SLEEP Duration (mean)','SLEEP Duration (std)')]:
            if s in X.columns: X[f'{pfx}_cv']=X[s]/X[m].clip(lower=0.01)
    X['log_glucose']=np.log(g); X['log_trig']=np.log(t)
    X['log_bmi']=np.log(b.clip(lower=1)); X['log_hdl']=np.log(h)
    X['log_homa_proxy']=np.log(g)+np.log(b.clip(lower=1))+np.log(t)-np.log(h)
    return X.fillna(0)

X_v7 = eng_v7(X_df[all_cols], all_cols).values
X_eng = engineer_all_features(X_df[all_cols], all_cols).values

xgb_d3 = dict(n_estimators=400, max_depth=3, learning_rate=0.03, subsample=0.55,
               colsample_bytree=0.57, min_child_weight=17, reg_alpha=0.49, reg_lambda=0.01)

def get_oof(model_fn, X, y_arr, splits, scale=False, target_fn=None, inv_fn=None, weights=None):
    oof_sum, oof_cnt = np.zeros(n), np.zeros(n)
    yt = target_fn(y_arr) if target_fn else y_arr
    for tr, te in splits:
        Xtr, Xte = X[tr], X[te]
        if scale: sc=StandardScaler(); Xtr=sc.fit_transform(Xtr); Xte=sc.transform(Xte)
        m = model_fn()
        if weights is not None:
            m.fit(Xtr, yt[tr], sample_weight=weights[tr])
        else:
            m.fit(Xtr, yt[tr])
        p = m.predict(Xte)
        if inv_fn: p = inv_fn(p)
        oof_sum[te] += p; oof_cnt[te] += 1
    return oof_sum / np.clip(oof_cnt, 1, None)

log_fn=np.log1p; inv_log=np.expm1
sqrt_fn=lambda y:np.sqrt(y.clip(min=0)); inv_sqrt=lambda p:p**2

oof_pool={}; scores={}; cnt=0
def add(name, oof):
    global cnt; cnt+=1
    r2=r2_score(y,oof); oof_pool[name]=oof; scores[name]=r2
    print(f"  [{cnt:2d}] {name:55s} R²={r2:.4f}"); sys.stdout.flush()

# ============================================================
# 1. FIND OPTIMAL WEIGHT EXPONENT
# ============================================================
print("\n--- 1. Weight Exponent Search (XGB d3 log v7) ---"); sys.stdout.flush()
for exp in [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]:
    w = y**exp / (y**exp).mean() if exp > 0 else None
    name = f'xgb_d3_log_w{exp:.1f}_v7'
    add(name, get_oof(lambda: xgb.XGBRegressor(**xgb_d3, random_state=2024, verbosity=0),
                       X_v7, y, splits, target_fn=log_fn, inv_fn=inv_log, weights=w))

# Find best exponent
best_exp = max([e for e in [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]],
               key=lambda e: scores[f'xgb_d3_log_w{e:.1f}_v7'])
print(f"\n  ★ Best exponent: {best_exp} → R²={scores[f'xgb_d3_log_w{best_exp:.1f}_v7']:.4f}")
sys.stdout.flush()

# ============================================================
# 2. APPLY BEST WEIGHT TO ALL MODEL TYPES
# ============================================================
print(f"\n--- 2. Best weight (y^{best_exp}) across all models ---"); sys.stdout.flush()
w_best = y**best_exp / (y**best_exp).mean()

# XGB variants
for seed in [42, 789]:
    add(f'xgb_d3_log_wopt_s{seed}_v7',
        get_oof(lambda s=seed: xgb.XGBRegressor(**xgb_d3, random_state=s, verbosity=0),
                X_v7, y, splits, target_fn=log_fn, inv_fn=inv_log, weights=w_best))
    add(f'xgb_d3_log_wopt_s{seed}_eng',
        get_oof(lambda s=seed: xgb.XGBRegressor(**xgb_d3, random_state=s, verbosity=0),
                X_eng, y, splits, target_fn=log_fn, inv_fn=inv_log, weights=w_best))

# XGB deep + weighted
add('xgb_d6_log_wopt_v7',
    get_oof(lambda: xgb.XGBRegressor(n_estimators=800, max_depth=6, learning_rate=0.01,
        subsample=0.6, colsample_bytree=0.5, min_child_weight=15, reg_alpha=0.1, reg_lambda=2.0,
        random_state=2024, verbosity=0), X_v7, y, splits, target_fn=log_fn, inv_fn=inv_log, weights=w_best))

# LGB weighted
add('lgb_d3_log_wopt_v7',
    get_oof(lambda: lgb.LGBMRegressor(n_estimators=500, max_depth=3, learning_rate=0.03,
        subsample=0.7, colsample_bytree=0.7, min_child_samples=20, verbose=-1, random_state=42),
     X_v7, y, splits, target_fn=log_fn, inv_fn=inv_log, weights=w_best))
add('lgb_d4_log_wopt_v7',
    get_oof(lambda: lgb.LGBMRegressor(n_estimators=400, max_depth=4, learning_rate=0.03,
        subsample=0.8, colsample_bytree=0.7, min_child_samples=15, verbose=-1, random_state=42),
     X_v7, y, splits, target_fn=log_fn, inv_fn=inv_log, weights=w_best))

# HGBR weighted (doesn't support sample_weight in fit — use raw, no target transform)
# Actually HGBR DOES support sample_weight
add('hgbr_d4_log_wopt_v7',
    get_oof(lambda: HistGradientBoostingRegressor(max_iter=500, max_depth=4,
        learning_rate=0.03, min_samples_leaf=10, random_state=42),
     X_v7, y, splits, target_fn=log_fn, inv_fn=inv_log, weights=w_best))

# ExtraTrees weighted
add('et200_wopt_v7',
    get_oof(lambda: ExtraTreesRegressor(n_estimators=200, random_state=42, n_jobs=1),
     X_v7, y, splits, weights=w_best))

# ============================================================
# 3. UNWEIGHTED BASELINES FOR BLEND DIVERSITY
# ============================================================
print("\n--- 3. Unweighted Baselines ---"); sys.stdout.flush()
add('xgb_d3_log_v7',
    get_oof(lambda: xgb.XGBRegressor(**xgb_d3, random_state=2024, verbosity=0),
            X_v7, y, splits, target_fn=log_fn, inv_fn=inv_log))
add('enet_01_eng',
    get_oof(lambda: ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000),
            X_eng, y, splits, scale=True))
add('enet_01_v7',
    get_oof(lambda: ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000),
            X_v7, y, splits, scale=True))
add('ridge_500_v7',
    get_oof(lambda: Ridge(alpha=500), X_v7, y, splits, scale=True))
add('lgb_d3_log_v7',
    get_oof(lambda: lgb.LGBMRegressor(n_estimators=500, max_depth=3, learning_rate=0.03,
        subsample=0.7, colsample_bytree=0.7, min_child_samples=20, verbose=-1, random_state=42),
     X_v7, y, splits, target_fn=log_fn, inv_fn=inv_log))

print(f"\n  Total: {cnt} models ({time.time()-t_start:.0f}s)")

# ============================================================
# 4. GREEDY BLEND + DIRICHLET
# ============================================================
print("\n--- 4. Greedy Forward + Dirichlet ---"); sys.stdout.flush()

sorted_m = sorted(scores, key=scores.get, reverse=True)
print("  Top 15:")
for i, nm in enumerate(sorted_m[:15], 1):
    print(f"    {i:2d}. {nm:55s} R²={scores[nm]:.4f}")

# Greedy forward
selected=[sorted_m[0]]; remaining=set(sorted_m[1:])
blend=oof_pool[selected[0]].copy(); cur_r2=scores[selected[0]]
print(f"\n  Greedy:")
print(f"    Step 1: {selected[0]:50s} R²={cur_r2:.4f}")
for step in range(2, 12):
    best_add=None; best_r2=cur_r2
    for nm in remaining:
        for alpha in np.arange(0.02, 0.50, 0.02):
            b=(1-alpha)*blend + alpha*oof_pool[nm]
            r2=r2_score(y,b)
            if r2>best_r2: best_r2=r2; best_add=nm; best_a=alpha
    if best_add is None or best_r2<=cur_r2+0.0001: break
    selected.append(best_add); remaining.discard(best_add)
    blend=(1-best_a)*blend+best_a*oof_pool[best_add]; cur_r2=best_r2
    print(f"    Step {step}: +{best_add:45s} α={best_a:.2f} → R²={cur_r2:.4f}")
    sys.stdout.flush()

# Dirichlet on greedy set
sel_oofs=np.column_stack([oof_pool[k] for k in selected])
best_dir=-999; rng=np.random.RandomState(42)
for _ in range(2000000):
    w=rng.dirichlet(np.ones(len(selected)))
    r2=1-np.sum((y-sel_oofs@w)**2)/np.sum((y-y.mean())**2)
    if r2>best_dir: best_dir=r2; best_w=w

print(f"\n  ★ Greedy: R²={cur_r2:.4f} ({len(selected)} models)")
print(f"  ★ Dirichlet on greedy: R²={best_dir:.4f}")
print("  Weights:")
for nm,w in zip(selected, best_w):
    if w>0.01: print(f"    {nm:55s} w={w:.3f}")

# Also try Dirichlet on top-15 overall
top15=sorted_m[:min(15,len(sorted_m))]
top15_oofs=np.column_stack([oof_pool[k] for k in top15])
best15=-999
for _ in range(2000000):
    w=rng.dirichlet(np.ones(len(top15)))
    r2=1-np.sum((y-top15_oofs@w)**2)/np.sum((y-y.mean())**2)
    if r2>best15: best15=r2
print(f"  ★ Top-15 Dirichlet: R²={best15:.4f}")

# ============================================================
# SUMMARY
# ============================================================
elapsed=time.time()-t_start
best_overall=max(cur_r2, best_dir, best15)
print(f"\n{'='*60}")
print(f"  V12 SUMMARY ({elapsed:.0f}s)")
print(f"{'='*60}")
print(f"  ★ BEST: R²={best_overall:.4f}")
print(f"  Best single: {sorted_m[0]} R²={scores[sorted_m[0]]:.4f}")
print(f"  Target: 0.65 | Gap: {0.65-best_overall:.4f}")

results = {
    'best_r2': float(best_overall),
    'best_single': {'name': sorted_m[0], 'r2': float(scores[sorted_m[0]])},
    'greedy_r2': float(cur_r2),
    'greedy_models': selected,
    'all_scores': {k: float(scores[k]) for k in sorted_m},
    'elapsed': elapsed
}
with open('v12_results.json','w') as f:
    json.dump(results,f,indent=2)
print(f"  Saved to v12_results.json"); sys.stdout.flush()
